<p><b><center>Crawling Educational Websites</center></b></p><br/>
  <p><b><center>Aakash Chandhoke<br> Motilal Nehru National Institute of Technology, Allahabad <br/>   aakash.chandhoke24@gmail.com</center></b></p><br/>
  <p><b><center>Akhilesh Kumar<br> Motilal Nehru National Institute of Technology, Allahabad <br/>    akhileshkumarhbti@gmail.com</center></b></p><br/>   <p><b><center>Shubham Kumar<br/> Motilal Nehru National Institute of Technology, Allahabad <br>    shubham.kumar.sci@gmail.com</center></b><p><br/>
<p><b>ABSTRACT</b></p><br/>
<p>Internet, the hub of capacious and diverse data, has become a vital part of our lives. With an immense amount of data available on the internet, web mining has emerged as a significant area of research due to its wide range applications. The term web mining was coined by Etzioni in the year of 1996[#singhweb]. Web mining mainly of three types -Web Content Mining, Web Structure Mining and Web Usage mining. Web Content Mining is the process of extracting useful information from the contents of Web documents. Web Structure Mining can be defined as the process of discovering structural information from the Web links and semantics. Web Usage Mining is the discovery of meaningful patterns from data generated by client-server transactions on one or more Web localities. This paper includes an overview of Web Mining and its application in crawling the semantic web for various topics and creating the database for the same. It extracts the links from the web and includes only the “.edu” links as an html file which can be used by the fellow researchers for their task.</p><br/>
<p><b>KEYWORDS</b></p><br/>
<p>Data Mining;Web Mining; Web Content Mining; Web Usage Mining; Web Structure Mining; Crawling; Topical Crawling.</p><br>
<p><b>INTRODUCTION</b></p><br/>
<p>A Web crawler, also known as a spider, is an Internet bot that systematically browses WWW, typically for the purpose of Web indexing (or web spidering). Web search engines and some other sites use Web crawling or spidering software to update their web content of others site's web content.<br>
Crawlers consume resources on the systems they visit and often visit sites without approval and authorization. Issues of “schedule”, “load”, and “politeness” come into play when large collections of HTML pages are accessed. Mechanisms exists for public sites not wishing to be crawled to make this known to the crawling agent[3].<br>
The number of pages on the internet is large, even the largest inter domain crawlers fall short of making a complete index. Only 40% of the HTML pages have been indexed by Page Rank Crawler (or the crawler used by Google). For that reason search engines were bad at giving relevant search
results in the early years of the WWW or before the year 2000. This is improved greatly by modern search engines; nowadays relevant results are given almost instantly due to large servers kept by tech giants like Google or Microsoft. Almost 90% of the data have been cached by Google (the pages they have crawled up-til now).<br></p>
<p><b>LITERATURE REVIEW</b></p><br/>
<p><b>DATA MINING</b></p><br/>
Data mining in layman terms means knowledge discovery from data. It is the extraction of interesting non trivial, previously un-known patterns and knowledge from the data. The essential differ-ence between the data mining and the traditional data analysis (such as query, reporting and on-line application of analysis) is that the data mining is to mine information and discover knowledge on the premise of no clear assumption[1]. There is another term similar to data mining that is: KDD (Knowledge Discovery in Database). Data mining is the core of Knowledge Discovery Process. Knowledge Dis-covery Process comprises of some other processes too, apart from Data Mining like: Data Cleaning, Data Integration, Data Selection, Data Transformation, Pattern Evaluation and Knowledge Represen-tation. Data Mining is somewhere sits between the Data Transfor-mation and Pattern Evaluation.Data mining commonly involves four classes of tasks: -Clustering , Classification , Regression , Association Rule Learning[2].<br/>
<p><b>Web Mining</b></p><br/>
<p>The internet has evolved into a global information space for almost all types of information that can be represented in various formats such as text, audio, video, graphics and animation. Internet is used for application that are used in all dimension of humankind, such as, in aviation, transportation, military and space information and ed-ucation etc. Due to phenomenal amount of data being kept on the WWW, it has eventually become the prime source of information. Millions of users navigate through the World Wide Web each day to retrieve useful information from it. To retrieve the best results from the enormous amount of data on the web, an extended part of data mining known as web mining is used[3]. Data mining requires retrieval of information from large structured databases and unstruc-tured databases, however, in web mining, the data and information is acquired from semi structured or unstructured web pages. Even for structured databases web mining can be used. Furthermore, when data mining of corporate information is done, the data is private and generally requires access rights to read.</p><br/>
<p><b>CLASSIFICATION</b></p><br/>
Web mining can be classified into three distinct areas namely Web Content Mining ,Web Structure Mining and Web usage Mining.<br/>
<p><b>WEB CONTENT MINING</b></p><br/>
<p>Web Content mining can be defined as, It is the technique of scan-ning and mining of text, pictures and graphs of Web page to de-termine the relevance of the content to the search query. With the
massive amount of information that is available on the World Wide Web. Web content mining provides results to search engines. Text Mining allows for the scanning of the entire WWW to retrieve the cluster content to activate the scanning of specific Web pages within those clusters. The results are pages relayed to the search engines through the highest level of relevance to the lowest[8].Text mining is very essential when used in relation to a database dealing with specific topics. For example online universities use a li-brary system to recall articles related to their general areas of study[7].</p><br/>
<p><b>WEB STRUCTURE MINING</b></p><br/>
<p>Web Structure Mining (WSM) is defined as the process of gener-ating structure of links of the web page in recent years. It discovers the hyperlink structure in inter documents level of the web page and finds the similarity and relationship among different web pages.The aim of Web Structure Mining is to generate abstract of the website and web pages.</p><br/>
<p><b>WEB USAGE MINING</b></p><br/>
<p>Web Usage Mining (WUM) is the discovery of patterns in asso-ciated data collected or generated when users access the websites . Since the mid-1990s, more than 400 papers have been published on Web Mining, out of which about 150 papers were published before 2000; out of these papers around 50% papers regarded Web Usage Mining. Web Usage Mining is falls under three stages (i) preprocess-ing, (ii) pattern discovery and (iii) pattern analysis.<br/>
Pattern discovery is representing methods and algorithms of var-ious different realms like statistics, data mining, machine learning and pattern recognition in the form of rules, tables, charts, graphs, or any other visual presentation forms for characterizing, categorizing or relating data from the web access logs[9]. Pattern analysis is the filtering of unnecessary patterns found in the pattern discovery stage. The most commonly used pattern analysis is in the form of SQL queries.</p><br/>
<p><b>WEB CRAWLERS</b></p><br/>
<p>A web crawler is a software that browses the World Wide Web in a systematic, automated manner[5]. The structure of the World Wide Web is a graphical structure, that is, the links presented in a web page may be used to open other web pages. Internet is a directed graph where web page as a node and hyperlink as an edge, thus the search operation may be summarized as a process of traversing directed graph. By following the linked structure of the Web, web crawler may traverse several new web pages starting from a web page. A web crawler move from links to links by the using of graphical structure of the web pages. Such programs are also known as robots, spiders, and worms. Web crawlers are designed to retrieve links and insert
them to local repository. Crawlers or Spiders are basically used to create a copy of all the visited links that are later processed by a search engine that will key the downloaded pages that help in quick searches.</p><br/>
<p><b>WORKING OF A WEB CRAWLER</b></p><br/>
<p>It has three main components: a frontier[4] which stores the list of URL’s to visit, Page Downloader which download pages from WWW and Web Repository receives web pages from a crawler and stores it in the database. The processes are as follows:</p><br/>
<p><b>CRAWLER FRONTIER</b></p><br/>
<p>It contains the list of unvisited URLs. The list is set with seed links delivered by a user or another program. Simply it’s just the collection of URLs. The working of the crawler starts with the seed URL or seed link. The crawler retrieves a URL from the frontier which contains the list of unvisited links. The page along with the link are fetched from the Web, and the links that are not visited from the page are added to the frontier. The cycle of fetching and extracting the links continues until the frontier is empty or some other condition causes it to stop. The extracting of URLs from the frontier is based on some prioritization rule.</p><br/>
<p><b>PAGE DOWNLOADER</b></p><br/>
<p>The main work of the page downloader is to download the page from the internet corresponding to the links which are retrieved from the crawler frontier. For that, the page downloader requires a HTTP client for sending the HTTP request and to read the response. There should be timeout period needs to set by the client in order to ensure that it will not take unnecessary time to read
large files or wait for response from slow server. In the actual implementation, the HTTP client is restricted to only download the first 10KB of a page[6].</p><br/>
<p><b>WEB REPOSITORY</b></p><br/>
<p>It use to stores and manages a large pool of data "objects," in case of crawler the object is web pages. The repository stores only standard HTML pages. All other types of documents and media files are ignored by the crawler. It is not different from other systems that store data objects, such as file systems, database management systems, or information retrieval systems. A web repository does not need to provide a lot of the functionality like other systems. It stores the crawled pages as different files and the storage stores the up-to-date version of every page retrieved by the crawler[3].</p><br/>
<p><b>SPIDER UNLEASHED</b></p><br/>
<p>The goal of this project is to create a web crawlers that could give the educational material related to a specified topic. There are various web crawlers and search engines available to perform such task but our goal is to specifically design a crawler that only gives results from “edu” websites and ignores the research papers associated with the topic, as research papers can be easily found on search engines like Google Scholar. The crawler introduced here provides a list of HTML web pages related to the given topic.</p><br/>
<p><b>METHODOLOGY</b></p><br/>
<p>The search on the web is a three step process viz. Where to start?, What and How to extract?, and When to stop?</p><br>
<p><b>WHERE TO START?</b></p><br/>
<p>To start a search on the web about any content, the first node from where the search is going to begin is necessary. Search engines use caching to store the results related to keywords while some websites traverse their files to provide the results.<br/>For our crawler to start the search, we have three options:<br>
1. Setup a dedicated server on a public IP which caches the educational websites by searching through their content in order to make a keyword and page mapping, then use an algorithm like Page Ranking to display more relevant results in sorted order.<br>
2. As most of the search engine does the aforementioned caching part already, So, instead of caching the websites and crawling them. We can just crawl on the search engines to get the results cached by them and them further filtering them out to satisfy our needs.<br>
3. Create a set of popular educational websites where the content is most likely to be found. And use their search queries to find the results on them.<br>To avoid unnecessary expenses on hardware and buying the public IP, we decided to go with the second option.</p><br>
<p><b>HOW AND WHAT TO EXTRACT?</b></p><br/>
<p>To extract the data from a search engine, a search query is to generated containing the information of the keyword to be searched along with some result specifications. This query is a string typically present in the URL bar of a web browser when a search is done on the search engine. After analysing various queries a generic pattern can be made to query the search engine without actually opening the search engine first. This process is already being used by most of the web browsers which provide a dedicated search box to search directly on a search engine.<br/>
After the query has been made, the search results are to be re-trieved and stored for further processing. We are going to use a pop-ular open-source web crawler HTTrack which is made to download the contents of a website for offline purpose.<br>
HTTrack does not provide any API which could have been used in programming to customize it’s functionality. However it provides a command-line version of itself which can be used to customize the options available in HTTrack. In order to use it in our programming, we created our own API (class) wrapped over command-line HTTrack that now can be used for programming purposes and specifying op-tions.
With the help of HTTrack now, the source code of the page con-taining search results provided by the search engine can now be re-trieved and stored.<br/>
To extract and filter out relevant URLs from the source code, some generic regular expressions are made to extract a list of URLs from the source code string. Although some filters can be applied in the search query itself, yet to enhance and provide better results. A second level filter is now applied on the list of URLs to filter out the results from “.edu” websites and also ignoring the PDF results from papers.</p><br/>
<p><b>WHEN TO STOP?</b></p><br/>
<p>As most of the search engines already use some version of page rank algorithm to provide the most relevant results first. The results that are retrieved after filtering are already sorted in most relevant order. The default number of searches are set to 60 in our crawler. However, user can override this in the GUI interface.</p><br>
<p><b>PROXY SERVER</b></p><br/>
<p>As, a lot of educational institutes use proxy servers in place of a regular NAT. The query to the search engine, and the source code of the page of results must be sent and received through the specific proxy server. To enhance the capability of our web crawler in order to work behind proxy servers we have used System.HTTP class available in Microsoft’s .NET.</p><br>
<p><b>RESULTS</b></p><br/>
<p>The final version of our crawlers features a GUI for the user to in-teract. The crawler takes a search string and number of results to be retrieved (which is by default is set to 60). The GUI also enables user to enter manual proxy settings.<br>
The speed of the search depends on the internet connection speed and also the search speed of the search engine. The crawler is tak-ing around 5 seconds on a regular internet connection to retrieve 60 results. The time may vary according to the internet speed and the number of results to be retrieved. The result is a list of URLs stored in a text file on the desktop of the user.</p><br>
<p><b>CONCLUSION</b></p><br/>
<p>We studied Web crawling at many different levels of web. Our main objectives were to develop a model for Web crawling, to study crawl-ing aspects and to build a Web crawler implementing them. This pa-per dealt with Web crawling from a practical point of view. From this point of view, we worked in a series of problems which appeared dur-ing the design and implementation of a Web crawler. We started by describing the topic of Data Mining and Web Mining, its taxonomy. Web also study the process of Web crawling and summarizing rele-vant literature on the topic. Although there are many studies about Web search, Web crawler designs and algorithms are mostly kept as business secret, with some exceptions. We discussed the structure of web crawler and the parts that are involved in its prime working.<br/>
We also proposed a method to crawl the “.edu” links such that our topic is relevant to be part of some “.edu” files over the internet. Thus our crawler crawls specific links of “.edu” pertaining to our query. During the crawling processes, we discovered and studied sev-eral practical issues that arise only after large crawls. Those issues are mostly anomalies in the implementations of the underlying protocols that form the Web, and must be taken into account when developing a Web crawler.</p><br>
<p><b>FUTURE ASPECTS</b></p><br/>
<p>After the standardization of HTML5 which provides more functional-ity of structuring of the content of web page, It is possible to retrieve only the specific part of the content on a web page. e.g. despite the page contains the headers, footers, images, headings etc., a paragraph in a page can be retrieved by extracting only the paragraph content specified by the HTML paragraph tag.
Unfortunately most of the web pages on the internet today still do not use utilize the enhanced structure functionality of HTML5. But in future, it is possible to modify this crawler to retrieve contents like headings, paragraphs etc. and to give the result in a more structured form like a book or an structured article.<br>The web crawler developed here is specific towards the educational websites and non-PDF results. Though it can be modified to fulfill other objectives. e.g. instead of “.edu” websites, a search can be made on other types of domain to get the results specific to those domains.</p><br/>
<p><b>REFERENCES</b></p><br/>
<p>[1] Soumen Chakrabarti. Mining the Web: Discovering knowledge from hyper-text data. Elsevier, 2002.<br/>
[2] Adil Fahad, Najlaa Alshatri, Zahir Tari, Abdullah Alamri, Ibrahim Khalil, Albert Y Zomaya, Sebti Foufou, and Abdelaziz Bouras. A survey of clus-tering algorithms for big data: Taxonomy and empirical analysis. IEEE transactions on emerging topics in computing, 2(3):267–279, 2014.<br/>
[3] Michael Goebel and Le Gruenwald. A survey of data mining and knowledge discovery software tools. ACM SIGKDD explorations newsletter, 1(1):20–33, 1999.<br/>
[4] Filippo Menczer, Gautam Pant, and Padmini Srinivasan. Topical web crawlers: Evaluating adaptive algorithms. ACM Transactions on Internet Technology (TOIT), 4(4):378–419, 2004.<br/>
[5] Seyed M Mirtaheri, Mustafa Emre Dinçtürk, Salman Hooshmand, Gregor V Bochmann, Guy-Vincent Jourdan, and Iosif Viorel Onut. A brief history of web crawlers. In Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research, pages 40–54. IBM Corp., 2013.<br/>
[6] Gautam Pant and Filippo Menczer. Myspiders: Evolve your own intelligent web crawlers. Autonomous agents and multi-agent systems, 5(2):221–229, 2002.<br/>
[7] Sriram Raghavan and Hector Garcia-Molina. Crawling the hidden web. Technical report, Stanford, 2000.<br/>
[8] Surabhi Singh, Devyani Gupta, and Aakash Chandhoke. Web mining: Tax-onomy and survey.<br/>
[9] Mike Thelwall. Link analysis: An information science approach. Emerald Group Publishing Limited, 2004.</p>[5] Seyed M Mirtaheri, Mustafa Emre Dinçtürk, Salman Hooshmand, Gregor V Bochmann, Guy-Vincent Jourdan, and Iosif Viorel Onut. A brief history of web crawlers. In Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research, pages 40–54. IBM Corp., 2013.</p><br/>





